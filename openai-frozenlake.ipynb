{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen lake\n",
    "\n",
    "Read environment description: \n",
    "https://gym.openai.com/envs/FrozenLake-v0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 0\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "state = env.reset()\n",
    "print('State:', state)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 0, True)\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "(state, reward, done, _) = env.step(2)\n",
    "print((state, reward, done))\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte-Carlo (v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate random sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode_random():\n",
    "    env = gym.make('FrozenLake-v0')\n",
    "    observations = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        (next_state, reward, done, _) = env.step(action)\n",
    "        observations.append([state, action, next_state, reward, done])\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    return observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 1, 0.0, False],\n",
       " [1, 3, 1, 0.0, False],\n",
       " [1, 1, 2, 0.0, False],\n",
       " [2, 3, 3, 0.0, False],\n",
       " [3, 0, 7, 0.0, True]]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_episode_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jurgis\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "counts = np.zeros((16, 4))\n",
    "sum_rewards =np.zeros((16, 4))\n",
    "\n",
    "for i in range(10000):\n",
    "    observations = run_episode_random()\n",
    "    last_reward = observations[-1][3]\n",
    "    for obs in observations:\n",
    "        state = obs[0]\n",
    "        action = obs[1]\n",
    "        counts[state, action] += 1\n",
    "        sum_rewards[state, action] += last_reward\n",
    "        \n",
    "q = sum_rewards/counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8262., 8227., 8057., 8410.],\n",
       "       [3170., 3191., 3132., 3262.],\n",
       "       [1475., 1430., 1427., 1436.],\n",
       "       [ 697.,  656.,  700.,  670.],\n",
       "       [3080., 3031., 3087., 3229.],\n",
       "       [   0.,    0.,    0.,    0.],\n",
       "       [ 472.,  429.,  427.,  414.],\n",
       "       [   0.,    0.,    0.,    0.],\n",
       "       [1180., 1133., 1157., 1132.],\n",
       "       [ 370.,  401.,  387.,  370.],\n",
       "       [ 224.,  229.,  240.,  228.],\n",
       "       [   0.,    0.,    0.,    0.],\n",
       "       [   0.,    0.,    0.,    0.],\n",
       "       [ 174.,  176.,  184.,  180.],\n",
       "       [ 125.,  150.,  144.,  140.],\n",
       "       [   0.,    0.,    0.,    0.]])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[121., 116., 115., 111.],\n",
       "       [ 27.,  44.,  35.,  54.],\n",
       "       [ 45.,  28.,  45.,  19.],\n",
       "       [  7.,   7.,   2.,   8.],\n",
       "       [ 72.,  41.,  53.,  20.],\n",
       "       [  0.,   0.,   0.,   0.],\n",
       "       [ 29.,  22.,  24.,   3.],\n",
       "       [  0.,   0.,   0.,   0.],\n",
       "       [ 16.,  45.,  37.,  52.],\n",
       "       [ 22.,  43.,  42.,  24.],\n",
       "       [ 53.,  45.,  43.,   7.],\n",
       "       [  0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.],\n",
       "       [ 14.,  34.,  43.,  30.],\n",
       "       [ 35.,  75.,  78.,  63.],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01464536, 0.01409991, 0.0142733 , 0.01319857],\n",
       "       [0.00851735, 0.01378878, 0.01117497, 0.01655426],\n",
       "       [0.03050847, 0.01958042, 0.03153469, 0.0132312 ],\n",
       "       [0.01004304, 0.01067073, 0.00285714, 0.0119403 ],\n",
       "       [0.02337662, 0.01352689, 0.01716877, 0.00619387],\n",
       "       [       nan,        nan,        nan,        nan],\n",
       "       [0.06144068, 0.05128205, 0.05620609, 0.00724638],\n",
       "       [       nan,        nan,        nan,        nan],\n",
       "       [0.01355932, 0.03971756, 0.03197926, 0.0459364 ],\n",
       "       [0.05945946, 0.10723192, 0.10852713, 0.06486486],\n",
       "       [0.23660714, 0.19650655, 0.17916667, 0.03070175],\n",
       "       [       nan,        nan,        nan,        nan],\n",
       "       [       nan,        nan,        nan,        nan],\n",
       "       [0.08045977, 0.19318182, 0.23369565, 0.16666667],\n",
       "       [0.28      , 0.5       , 0.54166667, 0.45      ],\n",
       "       [       nan,        nan,        nan,        nan]])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Act according to q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 0\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "state = env.reset()\n",
    "print('State:', state)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "action = np.argmax(q[state])\n",
    "(state, reward, done, _) = env.step(action)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode_q():\n",
    "    env = gym.make('FrozenLake-v0')\n",
    "    observations = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = np.argmax(q[state])\n",
    "        (next_state, reward, done, _) = env.step(action)\n",
    "        observations.append([state, action, next_state, reward, done])\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    return observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward:  0.018\n"
     ]
    }
   ],
   "source": [
    "# Random agent\n",
    "episodes = 1000\n",
    "sum = 0.\n",
    "for i in range(episodes):\n",
    "    observations = run_episode_random()\n",
    "    sum += observations[-1][3]\n",
    "print('Average reward: ', sum/episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward:  0.288\n"
     ]
    }
   ],
   "source": [
    "# Q-greedy agent\n",
    "episodes = 1000\n",
    "sum = 0.\n",
    "for i in range(episodes):\n",
    "    observations = run_episode_q()\n",
    "    sum += observations[-1][3]\n",
    "print('Average reward: ', sum/episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte-Carlo (v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.zeros((16, 4))\n",
    "sum_rewards =np.zeros((16, 4))\n",
    "epsilon = 0.1\n",
    "epochs = 0\n",
    "\n",
    "def act_eps_greedy(state):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, 3)\n",
    "    else:\n",
    "        return np.argmax(sum_rewards[state] / counts[state])\n",
    "\n",
    "def run_episode():\n",
    "    env = gym.make('FrozenLake-v0')\n",
    "    observations = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = act_eps_greedy(state)\n",
    "        (next_state, reward, done, _) = env.step(action)\n",
    "        observations.append([state, action, next_state, reward, done])\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    return observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 - Average reward:  0.297\n",
      "95 - Average reward:  0.317\n",
      "96 - Average reward:  0.282\n",
      "97 - Average reward:  0.314\n",
      "98 - Average reward:  0.316\n",
      "99 - Average reward:  0.292\n",
      "100 - Average reward:  0.32\n",
      "101 - Average reward:  0.3\n",
      "102 - Average reward:  0.31\n",
      "103 - Average reward:  0.294\n",
      "104 - Average reward:  0.325\n",
      "105 - Average reward:  0.308\n",
      "106 - Average reward:  0.292\n",
      "107 - Average reward:  0.305\n",
      "108 - Average reward:  0.3\n",
      "109 - Average reward:  0.281\n",
      "110 - Average reward:  0.303\n",
      "111 - Average reward:  0.308\n",
      "112 - Average reward:  0.301\n",
      "113 - Average reward:  0.291\n",
      "114 - Average reward:  0.312\n",
      "115 - Average reward:  0.291\n",
      "116 - Average reward:  0.313\n",
      "117 - Average reward:  0.309\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-322-0121284f232c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mobservations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_episode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mlast_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobservations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0msum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlast_reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-316-76beda6a330f>\u001b[0m in \u001b[0;36mrun_episode\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mact_eps_greedy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mobservations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-316-76beda6a330f>\u001b[0m in \u001b[0;36mact_eps_greedy\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum_rewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mcounts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrun_episode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[1;34m(a, axis, out)\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m     \"\"\"\n\u001b[1;32m-> 1037\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'argmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "\n",
    "    sum = 0.\n",
    "    episodes = 1000\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        observations = run_episode()\n",
    "        last_reward = observations[-1][3]\n",
    "        sum += last_reward\n",
    "        for obs in observations:\n",
    "            state = obs[0]\n",
    "            action = obs[1]\n",
    "            counts[state, action] += 1\n",
    "            sum_rewards[state, action] += last_reward\n",
    "\n",
    "    epochs += 1\n",
    "    print(epochs, '- Average reward: ', sum/episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=0.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
