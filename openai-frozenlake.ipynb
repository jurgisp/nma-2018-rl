{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen lake\n",
    "\n",
    "Read environment description: \n",
    "https://gym.openai.com/envs/FrozenLake-v0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 0\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "state = env.reset()\n",
    "print('State:', state)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.0, False)\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "(state, reward, done, _) = env.step(3)\n",
    "print((state, reward, done))\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte-Carlo (v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate random sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "\n",
    "def run_episode_random():\n",
    "    env = gym.make('FrozenLake-v0')\n",
    "    observations = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        (next_state, reward, done, _) = env.step(action)\n",
    "        observations.append([state, action, next_state, reward, done, 0.])\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "        \n",
    "    g = 0.\n",
    "    for i in reversed(range(len(observations))):\n",
    "        g = g * gamma + observations[i][3]\n",
    "        observations[i][5] = g\n",
    "    \n",
    "    return observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 4, 0.0, False, 0.0],\n",
       " [4, 2, 0, 0.0, False, 0.0],\n",
       " [0, 1, 4, 0.0, False, 0.0],\n",
       " [4, 2, 8, 0.0, False, 0.0],\n",
       " [8, 0, 12, 0.0, True, 0.0]]"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_episode_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jurgis\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "sum_rewards =np.zeros((16, 4))\n",
    "counts = np.zeros((16, 4))\n",
    "\n",
    "for i in range(10000):\n",
    "    observations = run_episode_random()\n",
    "    for obs in observations:\n",
    "        state = obs[0]\n",
    "        action = obs[1]\n",
    "        g = obs[5]\n",
    "        counts[state, action] += 1\n",
    "        sum_rewards[state, action] += g\n",
    "        \n",
    "q = sum_rewards/counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0148462 , 0.01254439, 0.01190693, 0.01202221],\n",
       "       [0.00505569, 0.00842471, 0.00777432, 0.01141634],\n",
       "       [0.01791014, 0.0165918 , 0.01894413, 0.00933774],\n",
       "       [0.00790739, 0.00855484, 0.00638967, 0.01219068],\n",
       "       [0.02350841, 0.01811445, 0.01387072, 0.01263384],\n",
       "       [       nan,        nan,        nan,        nan],\n",
       "       [0.04883013, 0.04321282, 0.03818029, 0.00239562],\n",
       "       [       nan,        nan,        nan,        nan],\n",
       "       [0.02065839, 0.04878105, 0.03393753, 0.05339776],\n",
       "       [0.08031372, 0.10282985, 0.11987337, 0.07174003],\n",
       "       [0.14430159, 0.14968003, 0.17355491, 0.05090913],\n",
       "       [       nan,        nan,        nan,        nan],\n",
       "       [       nan,        nan,        nan,        nan],\n",
       "       [0.06249295, 0.17137273, 0.25648404, 0.16876812],\n",
       "       [0.15170053, 0.52348244, 0.48350566, 0.35708213],\n",
       "       [       nan,        nan,        nan,        nan]])"
      ]
     },
     "execution_count": 605,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Act according to q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 0\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "state = env.reset()\n",
    "print('State:', state)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n"
     ]
    }
   ],
   "source": [
    "action = np.argmax(q[state])\n",
    "(state, reward, done, _) = env.step(action)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode_q():\n",
    "    env = gym.make('FrozenLake-v0')\n",
    "    observations = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = np.argmax(q[state])\n",
    "        (next_state, reward, done, _) = env.step(action)\n",
    "        observations.append([state, action, next_state, reward, done])\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    return observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward:  0.011\n"
     ]
    }
   ],
   "source": [
    "# Random agent\n",
    "episodes = 1000\n",
    "sum = 0.\n",
    "for i in range(episodes):\n",
    "    observations = run_episode_random()\n",
    "    sum += observations[-1][3]\n",
    "print('Average reward: ', sum/episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward:  0.092\n"
     ]
    }
   ],
   "source": [
    "# Q-greedy agent\n",
    "episodes = 1000\n",
    "sum = 0.\n",
    "for i in range(episodes):\n",
    "    observations = run_episode_q()\n",
    "    sum += observations[-1][3]\n",
    "print('Average reward: ', sum/episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte-Carlo (v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.zeros((16, 4))\n",
    "sum_rewards =np.zeros((16, 4))\n",
    "epsilon = 0.1\n",
    "gamma = 0.99\n",
    "epochs = 0\n",
    "\n",
    "def act_eps_greedy(state):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, 3)\n",
    "    else:\n",
    "        return np.argmax(sum_rewards[state] / counts[state])\n",
    "\n",
    "def run_episode():\n",
    "    env = gym.make('FrozenLake-v0')\n",
    "    observations = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = act_eps_greedy(state)\n",
    "        (next_state, reward, done, _) = env.step(action)\n",
    "        observations.append([state, action, next_state, reward, done, 0.])\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "        \n",
    "    g = 0.\n",
    "    for i in reversed(range(len(observations))):\n",
    "        g = g * gamma + observations[i][3]\n",
    "        observations[i][5] = g\n",
    "    \n",
    "    return observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jurgis\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in true_divide\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - Average reward:  0.087\n",
      "2 - Average reward:  0.272\n",
      "3 - Average reward:  0.34\n",
      "4 - Average reward:  0.369\n",
      "5 - Average reward:  0.385\n",
      "6 - Average reward:  0.375\n",
      "7 - Average reward:  0.351\n",
      "8 - Average reward:  0.351\n",
      "9 - Average reward:  0.353\n",
      "10 - Average reward:  0.355\n",
      "11 - Average reward:  0.364\n",
      "12 - Average reward:  0.357\n",
      "13 - Average reward:  0.373\n",
      "14 - Average reward:  0.362\n",
      "15 - Average reward:  0.375\n",
      "16 - Average reward:  0.378\n",
      "17 - Average reward:  0.38\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-697-d646a4e46267>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mobservations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_episode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mobs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mobservations\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-696-12ea1601b870>\u001b[0m in \u001b[0;36mrun_episode\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mact_eps_greedy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mobservations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\gym\\envs\\toy_text\\discrete.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mtransitions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcategorical_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\carnd-term1\\lib\\site-packages\\gym\\envs\\toy_text\\discrete.py\u001b[0m in \u001b[0;36mcategorical_sample\u001b[1;34m(prob_n, np_random)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprob_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mcsprob_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcsprob_n\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mnp_random\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "\n",
    "    sum = 0.\n",
    "    episodes = 1000\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        observations = run_episode()\n",
    "        for obs in observations:\n",
    "            state = obs[0]\n",
    "            action = obs[1]\n",
    "            g = obs[5]\n",
    "            counts[state, action] += 1\n",
    "            sum_rewards[state, action] += g\n",
    "            sum += obs[3]\n",
    "\n",
    "    epochs += 1\n",
    "    print(epochs, '- Average reward: ', sum/episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=0.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
